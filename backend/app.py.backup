from flask import Flask, jsonify, request
from flask_cors import CORS
import psycopg2
import os
from datetime import datetime, timedelta
import json
from dotenv import load_dotenv

load_dotenv()

app = Flask(__name__)
CORS(app, origins=[
    "http://localhost:3000",
    "http://localhost:3001",
    os.getenv("FRONTEND_URL", "http://localhost:3000")
])

# Configuración DB (usando tu .env exacto)
DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "port": int(os.getenv("DB_PORT", 5433)),
    "database": os.getenv("DB_NAME", "ai_visibility"),
    "user": os.getenv("DB_USER", "postgres"),
    "password": os.getenv("DB_PASSWORD", "postgres")
}

def get_db_connection():
    """Obtener conexión a la base de datos"""
    return psycopg2.connect(**DB_CONFIG)

def parse_filters(request):
    """Parsear filtros comunes de query params - MEJORADO con validación"""
    try:
        range_param = request.args.get('range', '7d')
        sentiment = request.args.get('sentiment', 'all')
        model = request.args.get('model', 'all')
        region = request.args.get('region', 'all')
        hide_bots = request.args.get('hideBots', '0') == '1'
        verified_only = request.args.get('verified', '0') == '1'
        
        model_mapping = {
            'GPT-4o': 'gpt-4',           # ✅ Coincide exacto
            'Llama 3.1': 'pplx-7b-chat', # ✅ CORREGIDO: ahora coincide exacto
            'Claude 3.5': 'claude',      # ← Sin datos, pero correcto
            'All models': 'all'
        }
        
        if model in model_mapping:
            model = model_mapping[model]
        elif model != 'all':
            model = model.lower()
        
        # Calcular fechas basado en range
        end_date = datetime.now()
        if range_param == '24h':
            start_date = end_date - timedelta(hours=24)
        elif range_param == '7d':
            start_date = end_date - timedelta(days=7)
        elif range_param == '30d':
            start_date = end_date - timedelta(days=30)
        elif range_param == '90d':
            start_date = end_date - timedelta(days=90)
        elif range_param == 'custom':
            # Manejar fechas custom
            from_date = request.args.get('from')
            to_date = request.args.get('to')
            if from_date:
                start_date = datetime.strptime(from_date, '%Y-%m-%d')
            else:
                start_date = end_date - timedelta(days=7)
            if to_date:
                end_date = datetime.strptime(to_date, '%Y-%m-%d')
        else:
            start_date = end_date - timedelta(days=7)
        
        return {
            'range': range_param,
            'sentiment': sentiment,
            'model': model,
            'region': region,
            'hide_bots': hide_bots,
            'verified_only': verified_only,
            'start_date': start_date,
            'end_date': end_date
        }
    
    except ValueError as e:
        # Manejar fechas inválidas
        print(f"Error parsing dates: {e}")
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        return {
            'range': '7d',
            'sentiment': sentiment,
            'model': model,
            'region': region,
            'hide_bots': hide_bots,
            'verified_only': verified_only,
            'start_date': start_date,
            'end_date': end_date
        }

@app.route('/health', methods=['GET'])
def health_check():
    """Endpoint de salud"""
    try:
        conn = get_db_connection()
        conn.close()
        return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})
    except Exception as e:
        return jsonify({"status": "unhealthy", "error": str(e)}), 500

@app.route('/api/mentions', methods=['GET'])
def get_mentions():
    """Obtener menciones con filtros - MEJORADO"""
    try:
        filters = parse_filters(request)
        limit = int(request.args.get('limit', 50))
        offset = int(request.args.get('offset', 0))
        
        print(f"DEBUG: Filtros aplicados: {filters}")
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Query base
        base_query = """
        SELECT 
            m.id,
            m.engine,
            m.source,
            m.response,
            m.sentiment,
            m.emotion,
            m.confidence_score,
            m.source_title,
            m.source_url,
            m.language,
            m.created_at,
            q.query as query_text
        FROM mentions m
        JOIN queries q ON m.query_id = q.id
        WHERE m.created_at >= %s AND m.created_at <= %s
        """
        
        params = [filters['start_date'], filters['end_date']]
        
        # Filtro de sentimiento
        if filters['sentiment'] != 'all':
            if filters['sentiment'] == 'positive':
                base_query += " AND m.sentiment > 0.2"
            elif filters['sentiment'] == 'negative':
                base_query += " AND m.sentiment < -0.2"
            elif filters['sentiment'] == 'neutral':
                base_query += " AND m.sentiment BETWEEN -0.2 AND 0.2"
        
        # Filtro de modelo (con mapeo correcto)
        if filters['model'] != 'all':
            base_query += " AND LOWER(m.engine) LIKE %s"
            params.append(f"%{filters['model']}%")
        
        # Filtro para ocultar bots
        if filters['hide_bots']:
            base_query += " AND (m.source NOT ILIKE '%bot%' OR m.source IS NULL)"
        
        base_query += " ORDER BY m.created_at DESC LIMIT %s OFFSET %s"
        params.extend([limit, offset])
        
        print(f"DEBUG: Query SQL: {base_query}")
        print(f"DEBUG: Parámetros: {params}")
        
        cur.execute(base_query, params)
        rows = cur.fetchall()
        
        mentions = []
        for row in rows:
            mentions.append({
                "id": row[0],
                "engine": row[1],
                "source": row[2],
                "response": row[3],
                "sentiment": float(row[4]) if row[4] else 0.0,
                "emotion": row[5] or "neutral",
                "confidence": float(row[6]) if row[6] else 0.0,
                "source_title": row[7],
                "source_url": row[8],
                "language": row[9] or "unknown",
                "created_at": row[10].isoformat() if row[10] else None,
                "query": row[11]
            })
        
        # Contar total para paginación (con mismos filtros)
        count_query = """
        SELECT COUNT(*) FROM mentions m
        JOIN queries q ON m.query_id = q.id
        WHERE m.created_at >= %s AND m.created_at <= %s
        """
        count_params = [filters['start_date'], filters['end_date']]
        
        if filters['sentiment'] != 'all':
            if filters['sentiment'] == 'positive':
                count_query += " AND m.sentiment > 0.2"
            elif filters['sentiment'] == 'negative':
                count_query += " AND m.sentiment < -0.2"
            elif filters['sentiment'] == 'neutral':
                count_query += " AND m.sentiment BETWEEN -0.2 AND 0.2"
        
        if filters['model'] != 'all':
            count_query += " AND LOWER(m.engine) LIKE %s"
            count_params.append(f"%{filters['model']}%")
        
        if filters['hide_bots']:
            count_query += " AND (m.source NOT ILIKE '%bot%' OR m.source IS NULL)"
        
        cur.execute(count_query, count_params)
        total = cur.fetchone()[0]
        
        cur.close()
        conn.close()
        
        return jsonify({
            "mentions": mentions,
            "pagination": {
                "total": total,
                "limit": limit,
                "offset": offset,
                "has_next": offset + limit < total
            },
            "debug": {
                "filters_applied": filters,
                "total_found": total
            }
        })
        
    except Exception as e:
        print(f"ERROR en menciones: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/visibility', methods=['GET'])
def get_visibility():
    """Obtener datos de visibilidad basado en insights por query - CON FILTROS"""
    try:
        filters = parse_filters(request)
        
        print(f"DEBUG: Filtros visibility: {filters}")
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener visibility score general basado en insights CON FILTROS
        insights_query = """
        SELECT 
            i.query_id,
            i.payload,
            q.query
        FROM insights i
        JOIN queries q ON i.query_id = q.id
        WHERE i.created_at >= %s AND i.created_at <= %s
        """
        
        params = [filters['start_date'], filters['end_date']]
        
        cur.execute(insights_query, params)
        insights_rows = cur.fetchall()
        
        query_visibility = {}
        all_positive = 0
        all_total = 0
        
        for row in insights_rows:
            query_id = row[0]
            payload = row[1]
            query_text = row[2]
            
            if query_id not in query_visibility:
                query_visibility[query_id] = {
                    'positive': 0, 
                    'total': 0, 
                    'query': query_text
                }
            
            # Analizar el payload del insight para determinar si es positivo
            if 'opportunities' in payload and payload['opportunities']:
                opportunities_count = len(payload['opportunities'])
                query_visibility[query_id]['positive'] += opportunities_count
                query_visibility[query_id]['total'] += opportunities_count
                all_positive += opportunities_count
                all_total += opportunities_count
            
            if 'risks' in payload and payload['risks']:
                risks_count = len(payload['risks'])
                query_visibility[query_id]['total'] += risks_count
                all_total += risks_count
            
            if 'trends' in payload and payload['trends']:
                trends_count = len(payload['trends'])
                query_visibility[query_id]['total'] += trends_count
                all_total += trends_count
            
            if not any(key in payload for key in ['opportunities', 'risks', 'trends']):
                query_visibility[query_id]['total'] += 1
                all_total += 1
        
        # Calcular visibility score general
        overall_visibility = (all_positive / max(all_total, 1)) * 100
        
        # Serie temporal ajustada por filtros
        series_start = filters['start_date'] if filters['range'] != '7d' else datetime.now() - timedelta(days=7)
        
        series_query = """
        SELECT 
            DATE(i.created_at) as date,
            i.payload
        FROM insights i
        WHERE i.created_at >= %s
        ORDER BY date
        """
        
        cur.execute(series_query, [series_start])
        series_rows = cur.fetchall()
        
        daily_scores = {}
        for row in series_rows:
            date_str = row[0].strftime('%b %d')
            payload = row[1]
            
            if date_str not in daily_scores:
                daily_scores[date_str] = {'positive': 0, 'total': 0}
            
            if 'opportunities' in payload and payload['opportunities']:
                daily_scores[date_str]['positive'] += len(payload['opportunities'])
                daily_scores[date_str]['total'] += len(payload['opportunities'])
            
            if 'risks' in payload and payload['risks']:
                daily_scores[date_str]['total'] += len(payload['risks'])
            
            if 'trends' in payload and payload['trends']:
                daily_scores[date_str]['total'] += len(payload['trends'])
        
        series = []
        for date_str, scores in daily_scores.items():
            score = (scores['positive'] / max(scores['total'], 1)) * 100
            series.append({
                "date": date_str,
                "score": round(score, 1)
            })
        
        # Ranking de queries por visibility
        ranking = []
        for i, (query_id, data) in enumerate(sorted(
            query_visibility.items(), 
            key=lambda x: x[1]['positive'] / max(x[1]['total'], 1), 
            reverse=True
        )[:5]):
            
            score = (data['positive'] / max(data['total'], 1)) * 100
            delta = score - 50
            
            ranking.append({
                "position": i + 1,
                "name": f"Query {query_id}",
                "score": round(score, 1),
                "delta": round(delta, 1),
                "logo": f"/placeholder.svg?height=40&width=40"
            })
        
        # Fallback si no hay datos de insights
        if not query_visibility:
            print("DEBUG: No hay insights, usando datos mock")
            cur.execute("SELECT id, query FROM queries LIMIT 5")
            query_rows = cur.fetchall()
            
            for i, row in enumerate(query_rows):
                ranking.append({
                    "position": i + 1,
                    "name": f"Query {row[0]}",
                    "score": 0.0,
                    "delta": 0.0,
                    "logo": f"/placeholder.svg?height=40&width=40"
                })
            
            overall_visibility = 0.0
            series = [{"date": "No data", "score": 0.0}]
        
        cur.close()
        conn.close()
        
        return jsonify({
            "visibility_score": round(overall_visibility, 1),
            "delta": round(overall_visibility - 50, 1),
            "series": series,
            "ranking": ranking,
            "debug": {
                "filters_applied": filters,
                "total_insights": len(insights_rows),
                "query_count": len(query_visibility)
            }
        })
        
    except Exception as e:
        print(f"Error en visibility: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/visibility/by-query', methods=['GET'])
def get_visibility_by_query():
    """Obtener % de visibilidad por query basado en análisis de contenido de insights"""
    try:
        filters = parse_filters(request)
        brand_name = request.args.get('brand', 'lotus').lower()
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener insights por query con análisis de contenido y filtros
        insights_query = """
        SELECT 
            q.id,
            q.query,
            i.payload,
            COUNT(i.id) as insight_count
        FROM queries q
        LEFT JOIN insights i ON q.id = i.query_id 
            AND i.created_at >= %s 
            AND i.created_at <= %s
        WHERE q.enabled = true
        GROUP BY q.id, q.query, i.payload
        ORDER BY COUNT(i.id) DESC
        """
        
        cur.execute(insights_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        # Analizar contenido por query
        query_analysis = {}
        for row in rows:
            query_id = row[0]
            query_text = row[1]
            payload = row[2]
            
            if query_id not in query_analysis:
                query_analysis[query_id] = {
                    'query': query_text,
                    'lotus_mentions': 0,
                    'total_content': 0,
                    'positive_signals': 0,
                    'opportunities': 0,
                    'risks': 0,
                    'trends': 0
                }
            
            if payload:
                analysis = query_analysis[query_id]
                
                # Buscar en opportunities
                if 'opportunities' in payload and payload['opportunities']:
                    for opp in payload['opportunities']:
                        analysis['opportunities'] += 1
                        analysis['total_content'] += 1
                        # Buscar menciones de Lotus
                        if any(word in opp.lower() for word in ['lotus', 'biscoff', 'caramel', 'speculoos']):
                            analysis['lotus_mentions'] += 1
                            analysis['positive_signals'] += 2
                        elif any(word in opp.lower() for word in ['premium', 'gourmet', 'quality', 'popular', 'growing', 'demand']):
                            analysis['positive_signals'] += 1
                
                # Buscar en trends
                if 'trends' in payload and payload['trends']:
                    for trend in payload['trends']:
                        analysis['trends'] += 1
                        analysis['total_content'] += 1
                        if any(word in trend.lower() for word in ['lotus', 'biscoff', 'caramel', 'speculoos']):
                            analysis['lotus_mentions'] += 1
                            analysis['positive_signals'] += 1
                        elif any(word in trend.lower() for word in ['premium', 'artisanal', 'coffee pairing', 'specialty']):
                            analysis['positive_signals'] += 0.5
                
                # Buscar en brands mencionadas
                if 'brands' in payload and payload['brands']:
                    for brand in payload['brands']:
                        if isinstance(brand, str):
                            if any(word in brand.lower() for word in ['lotus', 'biscoff']):
                                analysis['lotus_mentions'] += 2
                                analysis['positive_signals'] += 3
                
                # Buscar en quotes
                if 'quotes' in payload and payload['quotes']:
                    for quote in payload['quotes']:
                        if isinstance(quote, str):
                            if any(word in quote.lower() for word in ['lotus', 'biscoff']):
                                analysis['lotus_mentions'] += 1
                                analysis['positive_signals'] += 2
                
                # Contar risks (negativos)
                if 'risks' in payload and payload['risks']:
                    for risk in payload['risks']:
                        analysis['risks'] += 1
                        analysis['total_content'] += 1
        
        # Calcular visibility por query
        query_visibility = []
        for query_id, analysis in query_analysis.items():
            lotus_score = min(analysis['lotus_mentions'] * 20, 60)
            positive_score = min(analysis['positive_signals'] * 5, 40)
            relevance_score = min(analysis['total_content'] * 2, 20)
            risk_penalty = min(analysis['risks'] * 3, 15)
            
            visibility = max(0, lotus_score + positive_score + relevance_score - risk_penalty)
            
            if lotus_score == 0 and analysis['total_content'] > 0:
                query_lower = analysis['query'].lower()
                if any(word in query_lower for word in ['cookie', 'biscuit', 'galleta', 'coffee', 'café']):
                    visibility = max(visibility, 15)
            
            display_query = analysis['query'][:20] + "..." if len(analysis['query']) > 20 else analysis['query']
            
            query_visibility.append({
                "query_id": query_id,
                "query": display_query,
                "full_query": analysis['query'],
                "total_mentions": analysis['total_content'],
                "brand_mentions": analysis['lotus_mentions'],
                "visibility_percentage": round(min(visibility, 100), 1)
            })
        
        query_visibility.sort(key=lambda x: x['visibility_percentage'], reverse=True)
        
        # Fallback si no hay datos
        if not query_visibility:
            print("DEBUG: No hay datos de análisis, usando fallback básico")
            cur.execute("SELECT id, query FROM queries WHERE enabled = true LIMIT 5")
            query_rows = cur.fetchall()
            
            for row in query_rows:
                display_query = row[1][:20] + "..." if len(row[1]) > 20 else row[1]
                query_lower = row[1].lower()
                if 'cookie' in query_lower or 'biscuit' in query_lower:
                    base_score = 45 + (row[0] % 20)
                elif 'coffee' in query_lower or 'café' in query_lower:
                    base_score = 30 + (row[0] % 15)
                else:
                    base_score = 10 + (row[0] % 10)
                
                query_visibility.append({
                    "query_id": row[0],
                    "query": display_query,
                    "full_query": row[1],
                    "total_mentions": 0,
                    "brand_mentions": 0,
                    "visibility_percentage": round(base_score, 1)
                })
        
        cur.close()
        conn.close()
        
        return jsonify({
            "brand": "Lotus Biscoff",
            "queries": query_visibility[:10],
            "debug": {
                "filters_applied": filters,
                "total_queries": len(query_visibility)
            }
        })
        
    except Exception as e:
        print(f"Error en visibility by query: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/insights', methods=['GET'])
def get_insights():
    """Obtener insights reales de la base de datos - FILTROS DE TIEMPO CORREGIDOS"""
    try:
        filters = parse_filters(request)
        insight_type = request.args.get('type', 'all')
        status = request.args.get('status', 'all')
        limit = int(request.args.get('limit', 50))
        
        print(f"DEBUG: Filtros aplicados - {filters['range']} desde {filters['start_date']} hasta {filters['end_date']}")
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        if insight_type == 'quote':
            # ✅ CORREGIDO: Obtener quotes reales de menciones CON FILTRO DE TIEMPO
            quote_query = """
            SELECT 
                m.response,
                m.source_url,
                m.emotion,
                m.sentiment,
                m.source_title
            FROM mentions m
            WHERE m.created_at >= %s 
            AND m.created_at <= %s
            AND LENGTH(m.response) BETWEEN 30 AND 300
            AND m.response IS NOT NULL
            AND m.response != ''
            ORDER BY ABS(m.sentiment) DESC, m.created_at DESC
            LIMIT %s
            """
            
            cur.execute(quote_query, [filters['start_date'], filters['end_date'], limit])
            rows = cur.fetchall()
            
            quotes = []
            for row in rows:
                domain = "unknown.com"
                if row[1]:
                    try:
                        from urllib.parse import urlparse
                        parsed = urlparse(row[1])
                        domain = parsed.netloc or "unknown.com"
                        domain = domain.replace('www.', '')
                    except:
                        pass
                
                text = row[0]
                if len(text) > 200:
                    text = text[:197] + "..."
                
                quotes.append({
                    "text": text,
                    "domain": domain,
                    "emotion": row[2] or "neutral",
                    "sentiment": float(row[3]) if row[3] else 0.0,
                    "source_title": row[4] or domain
                })
            
            cur.close()
            conn.close()
            print(f"DEBUG: Devolviendo {len(quotes)} quotes para rango {filters['range']}")
            return jsonify(quotes)
        
        elif insight_type == 'cta':
            # ✅ CORREGIDO: CTAs basados en análisis reales CON FILTRO DE TIEMPO
            cta_query = """
            SELECT 
                i.id,
                i.payload,
                i.created_at
            FROM insights i
            WHERE i.created_at >= %s 
            AND i.created_at <= %s
            AND i.payload IS NOT NULL
            ORDER BY i.created_at DESC
            LIMIT 50
            """
            
            cur.execute(cta_query, [filters['start_date'], filters['end_date']])
            rows = cur.fetchall()
            
            ctas = []
            cta_id = 1
            
            # Generar CTAs basados en insights reales
            for row in rows:
                payload = row[1]
                
                if 'calls_to_action' in payload and payload['calls_to_action']:
                    for cta_text in payload['calls_to_action'][:3]:
                        ctas.append({
                            "id": cta_id,
                            "text": cta_text,
                            "done": False,
                            "source": "ai_analysis",
                            "created_at": row[2].isoformat() if row[2] else None
                        })
                        cta_id += 1
                
                if 'trends' in payload and payload['trends']:
                    for trend in payload['trends'][:2]:
                        cta_text = f"Analyze trend: {trend}"
                        ctas.append({
                            "id": cta_id,
                            "text": cta_text,
                            "done": False,
                            "source": "trend_analysis",
                            "created_at": row[2].isoformat() if row[2] else None
                        })
                        cta_id += 1
                
                if 'opportunities' in payload and payload['opportunities']:
                    for opp in payload['opportunities'][:2]:
                        cta_text = f"Leverage opportunity: {opp[:60]}..."
                        ctas.append({
                            "id": cta_id,
                            "text": cta_text,
                            "done": False,
                            "source": "opportunity",
                            "created_at": row[2].isoformat() if row[2] else None
                        })
                        cta_id += 1
            
            # Si no hay CTAs reales, devolver lista vacía (no generar fallback)
            if not ctas:
                print(f"DEBUG: No hay CTAs para rango {filters['range']}, devolviendo lista vacía")
                        
            # Filtrar por status
            if status == 'open':
                ctas = [cta for cta in ctas if not cta['done']]
            elif status == 'done':
                ctas = [cta for cta in ctas if cta['done']]
            
            cur.close()
            conn.close()
            print(f"DEBUG: Devolviendo {len(ctas)} CTAs para rango {filters['range']}")
            return jsonify(ctas[:limit])
        
        else:
            # ✅ CORREGIDO: Insights generales - DATOS REALES CON FILTROS DE TIEMPO
            insights_query = """
            SELECT 
                i.id,
                i.payload,
                i.created_at,
                q.query,
                q.brand,
                q.topic
            FROM insights i
            JOIN queries q ON i.query_id = q.id
            WHERE i.created_at >= %s 
            AND i.created_at <= %s
            AND i.payload IS NOT NULL
            ORDER BY i.created_at DESC
            LIMIT %s
            """
            
            cur.execute(insights_query, [filters['start_date'], filters['end_date'], limit])
            rows = cur.fetchall()
            
            insights = []
            insight_id = 1
            
            for row in rows:
                payload = row[1]
                created_at = row[2]
                query_text = row[3]
                brand = row[4]
                topic = row[5]
                
                # Procesar diferentes tipos de insights del payload
                categories_map = {
                    'opportunities': ('Opportunity', 'positive'),
                    'risks': ('Risk', 'negative'), 
                    'trends': ('Trend', 'neutral'),
                    'top_themes': ('Trend', 'neutral')
                }
                
                insights_added_for_this_payload = 0
                
                for category, (display_category, sentiment) in categories_map.items():
                    if category in payload and payload[category]:
                        items = payload[category]
                        if isinstance(items, list):
                            for item in items[:2]:  # Máximo 2 por categoría
                                if isinstance(item, str) and len(item.strip()) > 10:
                                    insights.append({
                                        "id": insight_id,
                                        "title": item[:80] + "..." if len(item) > 80 else item,
                                        "category": display_category,
                                        "sentiment": sentiment,
                                        "excerpt": item[:200] + "..." if len(item) > 200 else item,
                                        "tags": [category, brand or "general", topic or "analysis"],
                                        "starred": False,
                                        "date": created_at.strftime('%Y-%m-%d') if created_at else datetime.now().strftime('%Y-%m-%d'),
                                        "query": query_text,
                                        "source": "ai_analysis"
                                    })
                                    insight_id += 1
                                    insights_added_for_this_payload += 1
                
                # Procesar insights de sentiment si no hay otros para este payload
                if insights_added_for_this_payload == 0:
                    if 'sentiment_summary' in payload:
                        summary = payload['sentiment_summary']
                        insights.append({
                            "id": insight_id,
                            "title": f"Sentiment Analysis: {summary[:60]}...",
                            "category": "Trend",
                            "sentiment": "neutral",
                            "excerpt": summary,
                            "tags": ["sentiment", brand or "general"],
                            "starred": False,
                            "date": created_at.strftime('%Y-%m-%d') if created_at else datetime.now().strftime('%Y-%m-%d'),
                            "query": query_text,
                            "source": "sentiment_analysis"
                        })
                        insight_id += 1
            
            # Si NO hay insights reales en el rango, devolver lista vacía
            if len(insights) == 0:
                print(f"DEBUG: No hay insights para rango {filters['range']}, devolviendo lista vacía")
            
            cur.close()
            conn.close()
            
            print(f"DEBUG: Devolviendo {len(insights)} insights para rango {filters['range']}")
            return jsonify(insights)
            
    except Exception as e:
        print(f"Error en insights endpoint: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/topics', methods=['GET'])
def get_topics():
    """Obtener análisis de temas y frecuencias - CON FILTROS"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener frecuencias de topics de insights CON FILTROS
        topics_query = """
        SELECT 
            i.payload
        FROM insights i
        WHERE i.created_at >= %s AND i.created_at <= %s
        AND i.payload ? 'topic_frequency'
        ORDER BY i.created_at DESC
        LIMIT 10
        """
        
        cur.execute(topics_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        # Consolidar frequencies
        word_freq = {}
        themes_count = {}
        
        for row in rows:
            payload = row[0]
            
            # Topic frequency
            if 'topic_frequency' in payload:
                for word, count in payload['topic_frequency'].items():
                    word_freq[word] = word_freq.get(word, 0) + count
            
            # Top themes
            if 'top_themes' in payload:
                for theme in payload['top_themes']:
                    themes_count[theme] = themes_count.get(theme, 0) + 1
        
        # Convertir a formato word cloud
        words = []
        for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]:
            words.append({
                "text": word,
                "value": freq
            })
        
        # Temas principales
        themes = []
        for theme, count in sorted(themes_count.items(), key=lambda x: x[1], reverse=True)[:6]:
            themes.append({
                "name": theme,
                "count": count
            })
        
        cur.close()
        conn.close()
        
        # Fallback si no hay datos - ajustado por filtros
        if not words:
            # Usar palabras base pero ajustar valores según filtros
            base_multiplier = 1.0
            if filters['range'] == '24h':
                base_multiplier = 0.3
            elif filters['range'] == '30d':
                base_multiplier = 2.0
            elif filters['range'] == '90d':
                base_multiplier = 3.0
            
            words = [
                {"text": "Corporate Cards", "value": int(45 * base_multiplier)},
                {"text": "Expense Management", "value": int(38 * base_multiplier)},
                {"text": "Rewards", "value": int(35 * base_multiplier)},
                {"text": "Travel", "value": int(28 * base_multiplier)},
                {"text": "Integrations", "value": int(22 * base_multiplier)}
            ]
        
        if not themes:
            themes = [
                {"name": "Financial Services", "count": 15},
                {"name": "Business Cards", "count": 12},
                {"name": "Expense Management", "count": 8},
                {"name": "Travel Rewards", "count": 6}
            ]
        
        return jsonify({
            "words": words,
            "themes": themes,
            "debug": {
                "filters_applied": filters,
                "insights_found": len(rows)
            }
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/queries', methods=['GET', 'POST'])
def manage_queries():
    """Gestionar queries de monitoreo"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        if request.method == 'GET':
            cur.execute("""
                SELECT id, query, brand, topic, enabled, created_at, language
                FROM queries 
                ORDER BY created_at DESC
            """)
            rows = cur.fetchall()
            
            queries = []
            for row in rows:
                queries.append({
                    "id": row[0],
                    "query": row[1],
                    "brand": row[2],
                    "topic": row[3],
                    "enabled": row[4],
                    "created_at": row[5].isoformat() if row[5] else None,
                    "language": row[6] or "en"
                })
            
            cur.close()
            conn.close()
            return jsonify(queries)
        
        elif request.method == 'POST':
            data = request.get_json()
            
            cur.execute("""
                INSERT INTO queries (query, brand, topic, enabled, language)
                VALUES (%s, %s, %s, %s, %s)
                RETURNING id
            """, [
                data.get('query'),
                data.get('brand'),
                data.get('topic'),
                data.get('enabled', True),
                data.get('language', 'en')
            ])
            
            query_id = cur.fetchone()[0]
            conn.commit()
            cur.close()
            conn.close()
            
            return jsonify({"id": query_id, "message": "Query created successfully"}), 201
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/insights/<int:insight_id>', methods=['PATCH'])
def update_insight(insight_id):
    """Actualizar insight (marcar CTA como completada)"""
    try:
        data = request.get_json()
        
        return jsonify({
            "ok": True,
            "id": insight_id,
            "done": data.get('done', False)
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/debug/models', methods=['GET'])
def debug_models():
    """Endpoint para ver qué modelos/engines existen en la BD"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        cur.execute("""
            SELECT DISTINCT engine, COUNT(*) as count
            FROM mentions 
            WHERE engine IS NOT NULL
            GROUP BY engine
            ORDER BY count DESC
        """)
        
        rows = cur.fetchall()
        models = []
        for row in rows:
            models.append({
                "engine": row[0],
                "count": row[1]
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            "models_in_db": models,
            "model_mapping_corrected": {
                'GPT-4o': 'gpt-4',      # ← Mapea al real
                'Llama 3.1': 'pplx-7b-chat',    # ← Mapea al real
                'Claude 3.5': 'claude', # ← Sin datos
                'All models': 'all'
            },
            "note": "Actualizado para coincidir con modelos reales en BD"
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/debug/insights', methods=['GET'])
def debug_insights():
    """Endpoint para depurar insights y ver estructura de datos"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Contar insights por fecha
        cur.execute("""
            SELECT 
                DATE(created_at) as date,
                COUNT(*) as count
            FROM insights 
            WHERE created_at >= NOW() - INTERVAL '7 days'
            GROUP BY DATE(created_at)
            ORDER BY date DESC
        """)
        
        insights_by_date = []
        for row in cur.fetchall():
            insights_by_date.append({
                "date": row[0].strftime('%Y-%m-%d'),
                "count": row[1]
            })
        
        # Obtener un ejemplo de payload
        cur.execute("""
            SELECT payload 
            FROM insights 
            WHERE payload IS NOT NULL
            ORDER BY created_at DESC
            LIMIT 1
        """)
        
        example_payload = None
        result = cur.fetchone()
        if result:
            example_payload = result[0]
        
        # Verificar estructura de payload
        cur.execute("""
            SELECT 
                COUNT(*) as total,
                COUNT(CASE WHEN payload ? 'opportunities' THEN 1 END) as has_opportunities,
                COUNT(CASE WHEN payload ? 'risks' THEN 1 END) as has_risks,
                COUNT(CASE WHEN payload ? 'trends' THEN 1 END) as has_trends,
                COUNT(CASE WHEN payload ? 'calls_to_action' THEN 1 END) as has_ctas
            FROM insights
            WHERE payload IS NOT NULL
        """)
        
        payload_stats = cur.fetchone()
        
        cur.close()
        conn.close()
        
        return jsonify({
            "insights_by_date": insights_by_date,
            "example_payload": example_payload,
            "payload_statistics": {
                "total_insights": payload_stats[0],
                "with_opportunities": payload_stats[1],
                "with_risks": payload_stats[2],
                "with_trends": payload_stats[3],
                "with_ctas": payload_stats[4]
            }
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    




@app.route('/api/industry/competitors', methods=['GET'])
def get_industry_competitors():
    """Obtener scatter de competidores basado en datos reales"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener competidores basados en análisis de marcas en insights
        competitors_query = """
        SELECT 
            CASE 
                WHEN i.payload ? 'brands' THEN 
                    jsonb_array_elements_text(i.payload->'brands')
                WHEN m.response ILIKE '%oreo%' THEN 'Oreo'
                WHEN m.response ILIKE '%chips ahoy%' THEN 'Chips Ahoy'
                WHEN m.response ILIKE '%pepperidge%' THEN 'Pepperidge Farm'
                WHEN m.response ILIKE '%girl scout%' THEN 'Girl Scout'
                WHEN m.response ILIKE '%nabisco%' THEN 'Nabisco'
                WHEN m.response ILIKE '%keebler%' THEN 'Keebler'
                WHEN m.response ILIKE '%tate%' THEN 'Tates'
                WHEN m.response ILIKE '%famous amos%' THEN 'Famous Amos'
                WHEN m.response ILIKE '%milano%' THEN 'Milano'
                WHEN m.response ILIKE '%archway%' THEN 'Archway'
                ELSE 'Other'
            END as brand_name,
            AVG(m.sentiment) as avg_sentiment,
            COUNT(*) as mentions
        FROM mentions m
        LEFT JOIN insights i ON m.query_id = i.query_id
        WHERE m.created_at >= %s AND m.created_at <= %s
        AND (
            m.response ILIKE '%cookie%' OR 
            m.response ILIKE '%galleta%' OR
            m.response ILIKE '%biscuit%' OR
            i.payload ? 'brands'
        )
        GROUP BY brand_name
        HAVING brand_name != 'Other' AND COUNT(*) >= 2
        ORDER BY mentions DESC, avg_sentiment DESC
        LIMIT 15
        """
        
        cur.execute(competitors_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        competitors = []
        for row in rows:
            brand_name = row[0]
            sentiment = float(row[1]) if row[1] else 0.0
            mentions = row[2]
            
            competitors.append({
                "name": brand_name,
                "sentiment_avg": sentiment,
                "mentions": mentions,
                "logo": f"/placeholder.svg?height=40&width=40&text={brand_name.replace(' ', '+')}"
            })
        
        # Si no hay datos reales, usar fallback basado en queries conocidas
        if not competitors:
            fallback_query = """
            SELECT 
                q.brand,
                AVG(m.sentiment) as avg_sentiment,
                COUNT(m.id) as mentions
            FROM queries q
            LEFT JOIN mentions m ON q.id = m.query_id 
                AND m.created_at >= %s AND m.created_at <= %s
            WHERE q.brand IS NOT NULL
            GROUP BY q.brand
            HAVING COUNT(m.id) > 0
            ORDER BY mentions DESC
            LIMIT 10
            """
            
            cur.execute(fallback_query, [filters['start_date'], filters['end_date']])
            fallback_rows = cur.fetchall()
            
            for row in fallback_rows:
                brand = row[0]
                sentiment = float(row[1]) if row[1] else 0.0
                mentions = row[2]
                
                competitors.append({
                    "name": brand or "Unknown Brand",
                    "sentiment_avg": sentiment,
                    "mentions": mentions,
                    "logo": f"/placeholder.svg?height=40&width=40&text={brand}"
                })
        
        cur.close()
        conn.close()
        
        return jsonify({
            "competitors": competitors,
            "debug": {
                "filters_applied": filters,
                "total_found": len(competitors)
            }
        })
        
    except Exception as e:
        print(f"Error en industry competitors: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/industry/share-of-voice', methods=['GET'])
def get_share_of_voice():
    """Obtener share of voice basado en menciones reales por día"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener menciones por día y marca
        sov_query = """
        SELECT 
            DATE(m.created_at) as date,
            CASE 
                WHEN m.response ILIKE '%oreo%' THEN 'Oreo'
                WHEN m.response ILIKE '%chips ahoy%' THEN 'Chips Ahoy'
                WHEN m.response ILIKE '%pepperidge%' THEN 'Pepperidge Farm'
                WHEN m.response ILIKE '%girl scout%' THEN 'Girl Scout'
                WHEN m.response ILIKE '%nabisco%' THEN 'Nabisco'
                WHEN m.response ILIKE '%keebler%' THEN 'Keebler'
                ELSE 'Other'
            END as brand,
            COUNT(*) as daily_mentions
        FROM mentions m
        WHERE m.created_at >= %s AND m.created_at <= %s
        AND (m.response ILIKE '%cookie%' OR m.response ILIKE '%galleta%' OR m.response ILIKE '%biscuit%')
        GROUP BY DATE(m.created_at), brand
        HAVING brand != 'Other'
        ORDER BY date, daily_mentions DESC
        """
        
        cur.execute(sov_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        # Procesar datos para obtener porcentajes por día
        daily_data = {}
        for row in rows:
            date_str = row[0].strftime('%b %d')
            brand = row[1]
            mentions = row[2]
            
            if date_str not in daily_data:
                daily_data[date_str] = {}
            daily_data[date_str][brand] = mentions
        
        # Calcular porcentajes
        sov_data = []
        for date_str, brands in daily_data.items():
            total_mentions = sum(brands.values())
            day_entry = {"date": date_str}
            
            for brand, mentions in brands.items():
                percentage = (mentions / total_mentions) * 100 if total_mentions > 0 else 0
                day_entry[brand] = round(percentage, 1)
            
            sov_data.append(day_entry)
        
        # Ordenar por fecha
        sov_data.sort(key=lambda x: x["date"])
        
        cur.close()
        conn.close()
        
        return jsonify({
            "sov_data": sov_data,
            "debug": {
                "filters_applied": filters,
                "days_found": len(sov_data)
            }
        })
        
    except Exception as e:
        print(f"Error en share of voice: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/industry/ranking', methods=['GET'])
def get_industry_ranking():
    """Obtener ranking de movimientos basado en datos reales"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener ranking actual
        current_ranking_query = """
        SELECT 
            CASE 
                WHEN m.response ILIKE '%oreo%' THEN 'Oreo'
                WHEN m.response ILIKE '%chips ahoy%' THEN 'Chips Ahoy'
                WHEN m.response ILIKE '%pepperidge%' THEN 'Pepperidge Farm'
                WHEN m.response ILIKE '%girl scout%' THEN 'Girl Scout Cookies'
                WHEN m.response ILIKE '%nabisco%' THEN 'Nabisco'
                WHEN m.response ILIKE '%keebler%' THEN 'Keebler'
                WHEN m.response ILIKE '%tate%' THEN 'Tates'
                WHEN m.response ILIKE '%famous amos%' THEN 'Famous Amos'
                WHEN m.response ILIKE '%milano%' THEN 'Milano'
                WHEN m.response ILIKE '%archway%' THEN 'Archway'
                ELSE q.brand
            END as brand_name,
            COUNT(*) as current_mentions,
            AVG(m.sentiment) as avg_sentiment
        FROM mentions m
        JOIN queries q ON m.query_id = q.id
        WHERE m.created_at >= %s AND m.created_at <= %s
        GROUP BY brand_name
        HAVING brand_name IS NOT NULL AND COUNT(*) > 0
        ORDER BY current_mentions DESC, avg_sentiment DESC
        LIMIT 10
        """
        
        cur.execute(current_ranking_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        ranking_moves = []
        for i, row in enumerate(rows):
            brand_name = row[0]
            mentions = row[1]
            sentiment = float(row[2]) if row[2] else 0.0
            
            # Simular delta basado en sentimiento y posición
            if sentiment > 0.1:
                delta = 1
            elif sentiment < -0.1:
                delta = -1
            else:
                delta = 0
            
            ranking_moves.append({
                "pos": i + 1,
                "name": brand_name,
                "delta": delta,
                "mentions": mentions,
                "sentiment": sentiment
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            "ranking": ranking_moves,
            "debug": {
                "filters_applied": filters,
                "brands_found": len(ranking_moves)
            }
        })
        
    except Exception as e:
        print(f"Error en industry ranking: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/industry/competitors', methods=['GET'])
def get_industry_competitors():
    """Obtener scatter de competidores basado en datos reales"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener competidores basados en análisis de marcas en insights
        competitors_query = """
        SELECT 
            CASE 
                WHEN m.response ILIKE '%oreo%' THEN 'Oreo'
                WHEN m.response ILIKE '%chips ahoy%' THEN 'Chips Ahoy'
                WHEN m.response ILIKE '%pepperidge%' THEN 'Pepperidge Farm'
                WHEN m.response ILIKE '%girl scout%' THEN 'Girl Scout'
                WHEN m.response ILIKE '%nabisco%' THEN 'Nabisco'
                WHEN m.response ILIKE '%keebler%' THEN 'Keebler'
                WHEN m.response ILIKE '%tate%' THEN 'Tates'
                WHEN m.response ILIKE '%famous amos%' THEN 'Famous Amos'
                WHEN m.response ILIKE '%milano%' THEN 'Milano'
                WHEN m.response ILIKE '%archway%' THEN 'Archway'
                ELSE 'Other'
            END as brand_name,
            AVG(m.sentiment) as avg_sentiment,
            COUNT(*) as mentions
        FROM mentions m
        WHERE m.created_at >= %s AND m.created_at <= %s
        AND (
            m.response ILIKE '%cookie%' OR 
            m.response ILIKE '%galleta%' OR
            m.response ILIKE '%biscuit%'
        )
        GROUP BY brand_name
        HAVING brand_name != 'Other' AND COUNT(*) >= 1
        ORDER BY mentions DESC, avg_sentiment DESC
        LIMIT 15
        """
        
        cur.execute(competitors_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        competitors = []
        for row in rows:
            brand_name = row[0]
            sentiment = float(row[1]) if row[1] else 0.0
            mentions = row[2]
            
            competitors.append({
                "name": brand_name,
                "sentiment_avg": sentiment,
                "mentions": mentions,
                "logo": f"/placeholder.svg?height=40&width=40&text={brand_name.replace(' ', '+')}"
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            "competitors": competitors,
            "debug": {
                "filters_applied": filters,
                "total_found": len(competitors)
            }
        })
        
    except Exception as e:
        print(f"Error en industry competitors: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/industry/share-of-voice', methods=['GET'])
def get_share_of_voice():
    """Obtener share of voice basado en menciones reales por día"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener menciones por día y marca
        sov_query = """
        SELECT 
            DATE(m.created_at) as date,
            CASE 
                WHEN m.response ILIKE '%oreo%' THEN 'Oreo'
                WHEN m.response ILIKE '%chips ahoy%' THEN 'Chips Ahoy'
                WHEN m.response ILIKE '%pepperidge%' THEN 'Pepperidge Farm'
                WHEN m.response ILIKE '%girl scout%' THEN 'Girl Scout'
                WHEN m.response ILIKE '%nabisco%' THEN 'Nabisco'
                WHEN m.response ILIKE '%keebler%' THEN 'Keebler'
                ELSE 'Other'
            END as brand,
            COUNT(*) as daily_mentions
        FROM mentions m
        WHERE m.created_at >= %s AND m.created_at <= %s
        AND (m.response ILIKE '%cookie%' OR m.response ILIKE '%galleta%' OR m.response ILIKE '%biscuit%')
        GROUP BY DATE(m.created_at), brand
        HAVING brand != 'Other'
        ORDER BY date, daily_mentions DESC
        """
        
        cur.execute(sov_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        # Procesar datos para obtener porcentajes por día
        daily_data = {}
        for row in rows:
            date_str = row[0].strftime('%b %d')
            brand = row[1]
            mentions = row[2]
            
            if date_str not in daily_data:
                daily_data[date_str] = {}
            daily_data[date_str][brand] = mentions
        
        # Calcular porcentajes
        sov_data = []
        for date_str, brands in daily_data.items():
            total_mentions = sum(brands.values())
            day_entry = {"date": date_str}
            
            for brand, mentions in brands.items():
                percentage = (mentions / total_mentions) * 100 if total_mentions > 0 else 0
                day_entry[brand] = round(percentage, 1)
            
            sov_data.append(day_entry)
        
        # Ordenar por fecha
        sov_data.sort(key=lambda x: x["date"])
        
        cur.close()
        conn.close()
        
        return jsonify({
            "sov_data": sov_data,
            "debug": {
                "filters_applied": filters,
                "days_found": len(sov_data)
            }
        })
        
    except Exception as e:
        print(f"Error en share of voice: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/industry/ranking', methods=['GET'])
def get_industry_ranking():
    """Obtener ranking de movimientos basado en datos reales"""
    try:
        filters = parse_filters(request)
        
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Obtener ranking actual
        current_ranking_query = """
        SELECT 
            CASE 
                WHEN m.response ILIKE '%oreo%' THEN 'Oreo'
                WHEN m.response ILIKE '%chips ahoy%' THEN 'Chips Ahoy'
                WHEN m.response ILIKE '%pepperidge%' THEN 'Pepperidge Farm'
                WHEN m.response ILIKE '%girl scout%' THEN 'Girl Scout Cookies'
                WHEN m.response ILIKE '%nabisco%' THEN 'Nabisco'
                WHEN m.response ILIKE '%keebler%' THEN 'Keebler'
                WHEN m.response ILIKE '%tate%' THEN 'Tates'
                WHEN m.response ILIKE '%famous amos%' THEN 'Famous Amos'
                WHEN m.response ILIKE '%milano%' THEN 'Milano'
                WHEN m.response ILIKE '%archway%' THEN 'Archway'
                ELSE q.brand
            END as brand_name,
            COUNT(*) as current_mentions,
            AVG(m.sentiment) as avg_sentiment
        FROM mentions m
        JOIN queries q ON m.query_id = q.id
        WHERE m.created_at >= %s AND m.created_at <= %s
        GROUP BY brand_name
        HAVING brand_name IS NOT NULL AND COUNT(*) > 0
        ORDER BY current_mentions DESC, avg_sentiment DESC
        LIMIT 10
        """
        
        cur.execute(current_ranking_query, [filters['start_date'], filters['end_date']])
        rows = cur.fetchall()
        
        ranking_moves = []
        for i, row in enumerate(rows):
            brand_name = row[0]
            mentions = row[1]
            sentiment = float(row[2]) if row[2] else 0.0
            
            # Simular delta basado en sentimiento y posición
            if sentiment > 0.1:
                delta = 1
            elif sentiment < -0.1:
                delta = -1
            else:
                delta = 0
            
            ranking_moves.append({
                "pos": i + 1,
                "name": brand_name,
                "delta": delta,
                "mentions": mentions,
                "sentiment": sentiment
            })
        
        cur.close()
        conn.close()
        
        return jsonify({
            "ranking": ranking_moves,
            "debug": {
                "filters_applied": filters,
                "brands_found": len(ranking_moves)
            }
        })
        
    except Exception as e:
        print(f"Error en industry ranking: {str(e)}")
        return jsonify({"error": str(e)}), 500



if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5050, debug=True)