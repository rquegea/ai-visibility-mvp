# src/engines/openai_engine.py
"""
Wrapper de utilidades para la OpenAI Python >= 1.0.
Expone dos funciones:

    ‚Ä¢ fetch_response()    ‚Üí texto ‚Äúcrudo‚Äù del modelo
    ‚Ä¢ extract_insights()  ‚Üí JSON rico para dashboards

Todas las llamadas usan la nueva sintaxis v1 (`client.chat.completions.create`).
"""

from __future__ import annotations

import json
import logging
import os
from typing import Any, Dict

from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Funciones del Engine ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_response(
    prompt: str,
    *,
    model: str = "gpt-4o-mini",
    temperature: float = 0.3,
    max_tokens: int = 1_024,
) -> str:
    """
    Env√≠a un prompt y devuelve la respuesta textual del modelo.
    Usa gpt-4o-mini por defecto por ser r√°pido y econ√≥mico.
    """
    try:
        res = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "Eres un asistente √∫til. Sigue exactamente las instrucciones del usuario.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        answer: str = res.choices[0].message.content.strip()
        return answer
    except OpenAIError as exc:
        logger.exception("‚ùå OpenAI API error en fetch_response: %s", exc)
        raise


def extract_insights(text: str) -> Dict[str, Any]:
    """
    Analiza el CONTENIDO y devuelve un JSON listo para la tabla `insights`.
    Utiliza un modelo m√°s potente (gpt-4o) para asegurar alta calidad en el an√°lisis.
    """
    prompt = f"""
Eres un **analista senior de inteligencia de mercado**.

1Ô∏è‚É£ Lee atentamente el CONTENIDO.
2Ô∏è‚É£ Identifica **todas las MARCAS o productos** citados.
3Ô∏è‚É£ Cuenta cu√°ntas veces aparece cada marca.
4Ô∏è‚É£ Eval√∫a el **sentimiento promedio** hacia cada marca (‚àí1 a 1).
5Ô∏è‚É£ Detecta **competidores** relevantes.
6Ô∏è‚É£ Extrae **insights accionables** en: opportunities, risks, pain_points, trends.
7Ô∏è‚É£ A√±ade **hasta 3 QUOTES** literales (‚â§ 200 caracteres) que representen el tono.
8Ô∏è‚É£ Identifica los temas m√°s importantes (top_themes) y su frecuencia (topic_frequency).
9Ô∏è‚É£ Si se citan dominios (ej. forbes.com), an√≥talos en source_mentions.
üîü Extrae "calls_to_action", p√∫blico objetivo (audience_targeting) y productos/features.

Devuelve SOLO un objeto **JSON** con este formato EXACTO:

{{
  "brands": [{{"name": "...", "mentions": <int>, "sentiment_avg": <float>}}],
  "competitors": ["...", "..."],
  "opportunities": ["...", "..."],
  "risks": ["...", "..."],
  "pain_points": ["...", "..."],
  "trends": ["...", "..."],
  "quotes": ["...", "..."],
  "top_themes": ["...", "..."],
  "topic_frequency": {{"keyword": <int>}},
  "source_mentions": {{"domain": <int>}},
  "calls_to_action": ["...", "..."],
  "audience_targeting": ["...", "..."],
  "products_or_features": ["...", "..."]
}}

No a√±adas texto fuera del JSON.
----------
CONTENIDO:
{text}
----------
"""
    # Usamos gpt-4o expl√≠citamente para la m√°xima calidad en el an√°lisis
    raw = fetch_response(prompt, model="gpt-4o", temperature=0.2, max_tokens=2048)
    try:
        # Intenta limpiar la respuesta si viene en un bloque de c√≥digo markdown
        if raw.startswith("```json"):
            raw = raw[7:-3].strip()
        data: Dict[str, Any] = json.loads(raw)
        return data
    except (json.JSONDecodeError, TypeError) as exc:
        logger.error("‚ùå Error extrayendo insights: %s\nRespuesta del modelo: %s", exc, raw)
        return {}